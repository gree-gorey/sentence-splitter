# Отчет по проекту

1. [Аналитическая записка](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Аналитическая-записка)
  * [обзор существующих подходов](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-существующих-подходов)
  * [обзор проблем, пути решения](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-проблем-пути-решения)
  * [сложные случаи, с которыми пришлось бороться](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Сложные-случаи)
2. [Описание общей архитектуры системы](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Описание-общей-архитектуры-системы)
  * [формат входных и выходных данных](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Формат-входных-и-выходных-данных)
  * [основные этапы, компоненты](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Этапы-и-компоненты)
    * [графематическая нормализация](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Графематическая-нормализация)
    * [токенизация](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Токенизация)
    * [сегментация на предложения](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Сегментация-на-предложения)
    * [тестирование](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Тестирование)
  * [параметры](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Параметры)

## Аналитическая записка

#### Обзор существующих подходов

В [статье](http://www.dialog-21.ru/digests/dialog2008/materials/html/83.htm) Урюпиной предлагается подход к разбиению текстов на русском языке на предложения. Основная мысль ― существует омонимия знаков препинания, и, например, точка в русском языке может быть означающим не только конца предложения, но и сокращения, специального форматирования и т.д. В предыдущем предложении, кстати, точка в конце предложения выполняет сразу две функции: сигнализирует конец предложения и аббревиатуру. В работе Урюпиной был предложен метод разбиения на предложения с помощью машинного обучения. В набор вектора характеристик, по которым принимались решения, входил сам знак препинания, его контекст справа и слева, а также расстояние справа и слева до ближайшей потентиальной границы. Так как проведенные экперименты показали достаточно высокую точность и полноту, данный метод разбиения на предложения может иметь развитие.

#### Обзор проблем, пути решения

При токенизации мы сталкиваемся с проблемой поиска частей текста, являющимися границами токенов. Если мы разбиваем текст просто по пробелам, то у нас в токены попадут знаки препинания:
> сказал,<br>
> что

С другой стороны, некоторые токены не отделены пробелами, а вместо этого отделены только знаками препинания:
> больше/меньше

Если мы станем разбивать токены не только по пробелам, но и по знакам препинания, это приведет к тому, что некоторые токены окажутся разорванными:
> http<br>
> www<br>
> dialog<br>
> 21<br>
> ru<br>

Для решения этой проблемы необходимо понять, внутри каких классов токенов какие небуквенные символы мы можем встретить. Например, в сокращениях ― точку, в URL ― точку, слэш, двоеточие, нижнее подчеркивание и т.д. И далее обращать внимание на эти классы токенов при токенизации.

При сегментации на предложения мы сталкиваемся с омонимией точки. Кроме того, нужно осознавать, что знаки конца предложения `.!?` могут встречаться и внутри предложения.

#### Сложные случаи

Сложными случаями являются, например, те, когда точка выполняет сразу две функции. Например, у нас есть правило: точка + заглавная буква ― это конец предложения. Но тогда мы разделим предложения еще и в местах сокращений:
> Наш совхоз им.<br>
> Ленина прославился на весь округ.

При этом, если мы зададим список сокращений, после которых могут итти имена собственные, мы не сможем разбить такие предложения:
> Я все рассказал им. Ленина до сих пор не было.

Кроме того, знаки конца предложения (такие, как восклицательный знак) могут встретиться просто внутри предложения:
> В марте у нас три(!!) экзамена.

## Описание общей архитектуры системы

#### Формат входных и выходных данных

Входные данные ― текстовые файлы, очищенные от html-разметки

Выходные данные ― текстовые файлы, в которых все токены разделены с помощью `\n`, а предложения с помощью `\n\n`

#### Этапы и компоненты

###### Графематическая нормализация

В ходе графематической нормализации сначала тексты были очищены от html-разметки. Затем унифицировалась система пунктуации текста: все повторяющиеся знаки препинания заменялись на одиночные; разные формы одного и того же знака препинания заменялись на унифицированный вариант (например `"`, `«` или `“` для кавычек; `-` или `—` для тире и т.д.); все знаки препинания, являющиеся частью токена были заменены на символы, заведомо не способные встретиться в тексте (например, `ѧ` и `ꙋ`); все знаки препинания, не являющиеся часть токенов, отделялись с двух сторон пробелами:
> сказал , что

###### Токенизация

После успешно проведенной графематической нормализации этап токенизации состоял лишь в том, что весь текст был разделен по пробелам (поскольку все знаки препинания, не относящиеся к словам, были отделены на предыдущем этапе).

###### Сегментация на предложения

Сегментация на предложений состояла в обхождении токенов и принятия решения о том, является ли данный токен концом предложения, или нет. Условно схема работы программы представлена в виде псевдокода:
```python
for token in text:
  if token in '.!?':
    if next_token.istitle():
      end_of_sentence()
```
При этом все точки после инициалов и прочие случаи, не являющиеся концами предложений, были заменены на другие символами перед обходом текста.

###### Тестирование

Во-первых, было создан тестовый текст, содержащий сложные случаи. Этот текст был поделен на токены вручную (золотой стандарт) и с помощью программы. Тестирование токенизации состояло в том, что на основе полученных двух результатов были составлены частотные словари токенов. Эти словари сравнивались для оценки точности и полноты работы программы.

Для тестирования полноты и точности сегментации на предложения был составлен частотный словарь концов предложений на данных тех же файлов.

#### Параметры

У нас их не было.
