# Отчет по проекту

1. [Аналитическая записка](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Аналитическая-записка)
  * [обзор существующих подходов](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-существующих-подходов)
  * [обзор проблем, пути решения](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-проблем-пути-решения)
  * [сложные случаи, с которыми пришлось бороться](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Сложные-случаи)
2. [Описание общей архитектуры системы](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Описание-общей-архитектуры-системы)
  * [формат входных и выходных данных](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Формат-входных-и-выходных-данных)
  * [основные этапы, компоненты](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Этапы-и-компоненты)
    * [графематическая нормализация](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Графематическая-нормализация)
    * [токенизация](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Токенизация)
    * [сегментация на предложения](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Сегментация-на-предложения)
    * [тестирование](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Тестирование)
    * [анализ ошибок](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Анализ-ошибок)
  * [параметры](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Параметры)

## Аналитическая записка

#### Обзор существующих подходов

В [статье](http://www.dialog-21.ru/digests/dialog2008/materials/html/83.htm) О.М. Урюпиной обсуждается подход к разбиению текстов на русском языке на предложения. Основная мысль автора статьи состоит в том, что существует омонимия знаков препинания; например, точка в русском языке может быть означающим не только конца предложения, но и сокращения, специального форматирования и т.д. В предыдущем предложении, кстати, точка в конце предложения выполняет сразу две функции: сигнализирует о конце предложения и указывает на аббревиатуру. В работе О.М. Урюпиной был предложен метод разбиения текста на предложения с помощью машинного обучения. В набор вектора характеристик, по которым принимались решения, входил сам знак препинания, его контекст справа и слева, а также расстояние справа и слева до ближайшей потенциальной границы. Так как проведенные эксперименты показали достаточно высокую полноту и точность, данный метод разбиения на предложения может иметь развитие.

#### Обзор проблем, пути решения

При токенизации проблемой является поиск частей текста, являющихся границами токенов. Если использовать в качестве метода токенизации разбиение по пробелам, то знаки препинания также будут являться токенами:
> сказал,<br>
> что

С другой стороны, некоторые токены не отделены пробелами, а вместо этого отделены только знаками препинания:
> больше/меньше

Если использовать и пробелы, и знаки препинания как показатели сегментации, это приведет к тому, что некоторые токены окажутся разорванными:
> http<br>
> www<br>
> dialog<br>
> 21<br>
> ru<br>

Для решения этой проблемы необходимо понять, какие небуквенные символы могут встречаться внутри токенов. Например внутри URL можно встретить точку, слэш, двоеточие, нижнее подчеркивание и т. д. Далее следует обращать внимание на подобные классы токенов при токенизации.

При сегментации на предложения проблемой является омонимия точки. Кроме того, знаки конца предложения `.!?` могут встречаться и внутри предложения.

#### Сложные случаи

Сложными случаями являются, например, те, когда точка выполняет сразу две функции. Например, при наличии правила "точка + заглавная буква ― это конец предложения" произойдет разделение предложения в местах сокращений:
> Наш совхоз им.<br>
> Ленина прославился на весь округ.

При этом, если задать список сокращений, после которых могут идти имена собственные, возникнет ошибка при сегментации пердложений следующего типа:
> Я все рассказал им. Ленина до сих пор не было.

Кроме того, знаки конца предложения (такие, как восклицательный знак) могут встретиться и внутри предложения:
> В марте у нас три(!!) экзамена.

## Описание общей архитектуры системы

#### Формат входных и выходных данных

Входные данные ― это текстовые файлы, очищенные от html-разметки.

Выходные данные ― текстовые файлы, в которых все токены разделены с помощью `\n`, а предложения с помощью `\n\n`

#### Этапы и компоненты

###### Графематическая нормализация

В ходе графематической нормализации тексты были очищены от html-разметки. Затем унифицировалась система пунктуации текста: все повторяющиеся знаки препинания заменялись на одиночные; разные формы одного и того же знака препинания заменялись на унифицированный вариант (например `"`, `«` или `“` для кавычек; `-` или `—` для тире и т. д.); все знаки препинания, являющиеся частью токена были заменены на символы, заведомо не способные встретиться в тексте (например, `ѧ` и `ꙋ`); все знаки препинания, не являющиеся частью токенов, отделялись с двух сторон пробелами:
> сказал , что

###### Токенизация

После успешно проведенной графематической нормализации этап токенизации состоял лишь в том, что весь текст был разделен по пробелам (поскольку все знаки препинания, не относящиеся к словам, были отделены на предыдущем этапе).

###### Сегментация на предложения

Сегментация на предложения состояла в обхождении токенов и принятии решения о том, является ли данный токен концом предложения. Условно схема работы программы представлена в виде псевдокода:
```python
for token in text:
  if token in '.!?':
    if next_token.istitle():
      end_of_sentence()
```
При этом после инициалов и в других позициях, где точки не являются идентификатором конца предложения, они были заменены на другие символы перед обходом текста.

###### Тестирование

В первую очередь был создан тестовый текст, содержащий сложные случаи. Этот текст был поделен на токены вручную (золотой стандарт) и с помощью программы. Тестирование токенизации состояло в том, что на основе полученных двух результатов были составлены частотные словари токенов. Эти словари сравнивались для оценки точности и полноты работы программы.

Для тестирования полноты и точности сегментации на предложения был составлен частотный словарь концов предложений на данных тех же файлов.

Результаты:
Токенизация:
* точность — 93%
* полнота — 91%

Сегментация на предложения:
* точность — 93%
* полнота — 82%

###### Анализ ошибок

После тестирования был проведен анализ ошибок и исправление некоторых компонентов программы.
Ошибки токенизации:
* не учтены неразрывные пробелы
* не все знаки препинания в графематической нормализации отделяются от слов
* в выходном файле не заменялись обратно технические символы, добавленные в процессе графематической нормализации

Ошибки сегментации на предложения:
* символ переноса строки давал *False positive*

#### Параметры

У нас их не было.
