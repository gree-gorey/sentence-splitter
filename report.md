# Отчет по проекту

1. [Аналитическая записка](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Аналитическая-записка)
  * [обзор существующих подходов](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-существующих-подходов)
  * [обзор проблем, пути решения](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Обзор-проблем,-пути-решения)
  * [сложные случаи, с которыми пришлось бороться](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Сложные-случаи)
2. [Описание общей архитектуры системы](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Описание-общей-архитектуры-системы)
  * [формат входных и выходных данных](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Формат-входных-и-выходных-данных)
  * [основные этапы, компоненты](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Этапы-и-компоненты)
  * [параметры](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Параметры)

## Аналитическая записка

#### Обзор существующих подходов

В [статье](http://www.dialog-21.ru/digests/dialog2008/materials/html/83.htm) Урюпиной предлагается подход к токенизации.

#### Обзор проблем, пути решения

При токенизации мы сталкиваемся с проблемой поиска частей текста, являющимися границами токенов. Если мы разбиваем текст просто по пробелам, то у нас в токены попадут знаки препинания:
> сказал,<br>
> что

С другой стороны, некоторые токены не отделены пробелами, а вместо этого отделены только знаками препинания:
> больше/меньше

Если мы станем разбивать токены не только по пробелам, но и по знакам препинания, это приведет к тому, что некоторые токены окажутся разорванными:
> http<br>
> www<br>
> dialog-21<br>
> ru<br>



#### Сложные случаи

  1. обзор существующих подходов к токенизации и разбиение на предложения: методы, решения
    * анализ одной из статей
    * анализ работы выбранной вами системы или решений, представленных в выбранном вами корпусе
  2. обзор проблем, которые встречаются при разбиении текста на предложения и токены, предложенные пути решения
  3. основные сложные случаи, с которыми вам пришлось бороться. Ваши предложения с анализом сложных случаев (например, слова с дефисом – как получить список слов с дефисом, которые нужно учитывать при токенизации, где взять список сокращений, которые нужно учитывать при разбиении текста на предложения)

## Описание общей архитектуры системы
  1. формат входных и выходных данных
    * входные данные ― текстовые файлы, очищенные от html-разметки
    * выходные данные ― текстовые файлы, в которых все токены разделены с помощью `\n`, а предложения с помощью `\n\n`
  2. [основные этапы, компоненты](https://github.com/gree-gorey/sentence_splitter/blob/master/report.md#Этапы-и-компоненты)
  3. параметры: что подается как параметр для системы (необходимые списки, шаблоны и т.п.)


#### Формат входных и выходных данных

Входные данные ― текстовые файлы, очищенные от html-разметки
Выходные данные ― текстовые файлы, в которых все токены разделены с помощью `\n`, а предложения с помощью `\n\n`

#### Этапы и компоненты

###### Графематическая нормализация

###### Токенизация

###### Сегментация на предложения

###### Тестирование


#### Параметры

все
